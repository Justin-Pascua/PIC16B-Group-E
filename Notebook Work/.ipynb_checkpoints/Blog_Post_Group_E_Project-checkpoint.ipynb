{
 "cells": [
  {
   "cell_type": "raw",
   "id": "107fff1c-4527-4fd2-b239-5179155211f9",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Creating Machine Learning Models to Predict the Stock Market\"\n",
    "author: \n",
    "      - \"Justin Pascua\"\n",
    "      - \"Joshua Antilla\"\n",
    "      - \"Shuma Jensen\"\n",
    "date: \"2025-3-21\"\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eed04d-ad2f-4bf2-9db8-86e6eed36f9e",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee3a3f5-4481-428f-a161-63e6dcf8b1f0",
   "metadata": {},
   "source": [
    "In this post, we will discuss how to use data gathered from the Polygon and Yahoo Finance APIs to train machine learning models that will predict prospective features of the stock market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783beeb-aa20-407b-b6fb-990aeabd1447",
   "metadata": {},
   "source": [
    "For the purposes of this post, we will assume that the user has access to the .py files found in our github repository. However, we will still include important code from these files to explain how the models work. We will begin by importing important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaba06b3-9292-46b6-bafc-c67e01cf5cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c596a870-fe27-4dc1-9f8c-974eb7911565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the directory containing the .py files\n",
    "module_path = os.path.abspath(os.path.join('..')) # Use relative or absolute path. '..' means one level up.\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca90491-3f2b-4450-85d3-d37f859005f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polygon_interface\n",
    "import yahoo_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5118929d-e55e-4bb7-87d3-aa0fbbd66539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polygon-api-client in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (1.14.3)\n",
      "Requirement already satisfied: certifi<2026.0.0,>=2022.5.18 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from polygon-api-client) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.9 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from polygon-api-client) (1.26.20)\n",
      "Requirement already satisfied: websockets<15.0,>=10.3 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from polygon-api-client) (10.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install polygon-api-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e59ca4a-62bd-4f17-b071-20cdca59d897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_decision_forests in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (1.26.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (2.0.3)\n",
      "Requirement already satisfied: tensorflow==2.18.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (2.18.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (2.1.0)\n",
      "Requirement already satisfied: wheel in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (0.44.0)\n",
      "Requirement already satisfied: wurlitzer in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (3.1.1)\n",
      "Requirement already satisfied: tf-keras~=2.17 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (2.18.0)\n",
      "Requirement already satisfied: ydf in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow_decision_forests) (0.10.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (74.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.9.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from pandas->tensorflow_decision_forests) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from pandas->tensorflow_decision_forests) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from pandas->tensorflow_decision_forests) (2024.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/PIC16B-25W/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_decision_forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b683cec0-9db6-4db1-8df8-3d564119e74c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc82fdf-9e0e-41d8-9923-d4f2c83a55a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"margin:0px;\">ðŸŒ² Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
       "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
       "        Decision Forests</a> using the same algorithms but with more features and faster\n",
       "    training!\n",
       "</p>\n",
       "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
       "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
       "        <p\n",
       "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
       "            Old code</p>\n",
       "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
       "import tensorflow_decision_forests as tfdf\n",
       "\n",
       "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
       "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
       "model.fit(tf_ds)\n",
       "</pre>\n",
       "    </div>\n",
       "    <div style=\"width: 5px;\"></div>\n",
       "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
       "        <p\n",
       "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
       "            New code</p>\n",
       "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
       "import ydf\n",
       "\n",
       "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
       "</pre>\n",
       "    </div>\n",
       "</div>\n",
       "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
       "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
       "        guide</a>)</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import CNN_Maker\n",
    "import LSTM_Maker\n",
    "import Random_Forest_Maker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eaa014-eb4d-45b9-b49d-382be74e7502",
   "metadata": {},
   "source": [
    "Note: most of the necessary code for creating the models is found in these imported modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ec31f-318f-45f7-8ac4-ab52ba42694f",
   "metadata": {},
   "source": [
    "# Acquiring and Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a037bb9-ac2a-4316-a1a4-1f4ae77a1938",
   "metadata": {},
   "source": [
    "The following functions demonstrate how we obtain the data we need from the Polygon and Yahoo APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0409a-b885-446c-b040-df57c27f2485",
   "metadata": {},
   "source": [
    "First we have methods to create a table of data for a single stock, and methods to format the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca7b719-cacb-4b11-a5d8-9fd2cd093775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def single_stock_table(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Returns a dataframe containing all available stock data from Yahoo Finance \n",
    "    for a given stock between start_date and end_date.\n",
    "    Dates are in the format YYYY-MM-DD.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch stock data\n",
    "    df = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "    # Reset index to get the Date as a column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename columns to match the original function (except VWAP)\n",
    "    df.rename(columns={\n",
    "        'Date': 'Date',\n",
    "        'Open': 'Open',\n",
    "        'High': 'High',\n",
    "        'Low': 'Low',\n",
    "        'Close': 'Close',\n",
    "        'Volume': 'Volume'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Convert the Date column to timestamps (milliseconds)\n",
    "    df['Timestamp'] = df['Date'].astype('str').apply(date_to_timestamp)  # Convert to milliseconds\n",
    "    \n",
    "    # Rearrange columns (excluding VWAP)\n",
    "    # Add this line inside the function to compute VWAP before rearranging columns\n",
    "    df['Vwap'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    df = df[['Timestamp', 'Date', 'Close', 'Volume', 'Open', 'High', 'Low', 'Vwap']]\n",
    "\n",
    "    df = flatten_columns(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "690a3ffd-38f3-4589-91ed-fb0c4b8662a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_columns(df):\n",
    "    \"\"\"Flattens MultiIndex column names by taking top-most name.\"\"\"\n",
    "    df.columns = df.columns.get_level_values(0)\n",
    "    df.columns.name = None\n",
    "    df.index.name = None\n",
    "    return df\n",
    "\n",
    "def exponential_smoothing(a, stock_data):\n",
    "    \"\"\"\n",
    "    Applies exponential smoothing to each column (except timestamp and date) \n",
    "    of the dataframe\n",
    "    0 < alpha < 1\n",
    "    At alpha = 1, the smoothed data is equal to the initial data\n",
    "    \"\"\"\n",
    "    stock_data[['Close', 'Volume', 'Open', 'High', 'Low']] = stock_data[['Close', 'Volume', 'Open', 'High', 'Low']].ewm(alpha = a, adjust = True).mean()\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bd598-0941-4ea6-9e23-15693c68cbd9",
   "metadata": {},
   "source": [
    "The following functions extract important indicators of stock trends and the rise or fall of prices: MACD, RSI, Williams %R, and KDJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42ae9be-5d52-4275-803b-e63c1bc9ebeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_macd(stock_ticker, start_date, end_date, short_window=12, long_window=26, signal_window=9):\n",
    "    \"\"\"\n",
    "    Fetch MACD data for a given stock using yfinance.\n",
    "\n",
    "    - If MACD > Signal Line â†’ Upward trend (potential buy signal)\n",
    "    - If MACD < Signal Line â†’ Downward trend (potential sell signal)\n",
    "    \n",
    "    Parameters:\n",
    "    - stock_ticker: Stock symbol (e.g., \"AAPL\")\n",
    "    - start_date, end_date: Date range (YYYY-MM-DD)\n",
    "    - short_window: Short-term EMA period (default=12)\n",
    "    - long_window: Long-term EMA period (default=26)\n",
    "    - signal_window: Signal line EMA period (default=9)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch stock data from Yahoo Finance\n",
    "    df = yf.download(stock_ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "\n",
    "    # Compute short-term and long-term exponential moving averages (EMA)\n",
    "    df['Short_EMA'] = df['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "    df['Long_EMA'] = df['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "    \n",
    "    # Calculate MACD Line\n",
    "    df['MACD Value'] = df['Short_EMA'] - df['Long_EMA']\n",
    "\n",
    "    # Calculate Signal Line (9-day EMA of MACD)\n",
    "    df['Signal'] = df['MACD Value'].ewm(span=signal_window, adjust=False).mean()\n",
    "    \n",
    "    # Convert Date index to a column and format\n",
    "    df.reset_index(inplace=True)\n",
    "        \n",
    "    # Add timestamp column\n",
    "    df['Timestamp'] = df['Date'].astype(str).apply(date_to_timestamp)\n",
    "        \n",
    "    # Select relevant columns\n",
    "    df = df[['Timestamp', 'Date', 'MACD Value', 'Signal']]\n",
    "    \n",
    "    df = df.iloc[long_window - 1:]\n",
    "        \n",
    "    df = flatten_columns(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edb9db8-32d8-46ea-a1a5-6f40bce5211f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_rsi(stock_ticker, start_date, end_date, window=14):\n",
    "    \"\"\"\n",
    "    Fetches the Relative Strength Index (RSI) for a given stock ticker using yfinance.\n",
    "    \n",
    "    - RSI > 70 â†’ Overbought (potential price drop)\n",
    "    - RSI < 30 â†’ Oversold (potential price increase)\n",
    "    \n",
    "    Parameters:\n",
    "    - stock_ticker: Ticker symbol (e.g., \"AAPL\")\n",
    "    - start_date, end_date: Date range (YYYY-MM-DD)\n",
    "    - window: Period for RSI calculation (default = 14)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with RSI values\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch stock data\n",
    "    df = yf.download(stock_ticker, start=start_date, end=end_date, interval=\"1d\")\n",
    "\n",
    "    # Calculate price change\n",
    "    df[\"Price Change\"] = df[\"Close\"].diff()\n",
    "\n",
    "    # Calculate gains and losses\n",
    "    df[\"Gain\"] = df[\"Price Change\"].apply(lambda x: x if x > 0 else 0)\n",
    "    df[\"Loss\"] = df[\"Price Change\"].apply(lambda x: -x if x < 0 else 0)\n",
    "\n",
    "    # Calculate rolling average gains and losses\n",
    "    avg_gain = df[\"Gain\"].rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = df[\"Loss\"].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "    # Compute Relative Strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "    df[\"RSI Value\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Convert Date index to a column and format\n",
    "    df.reset_index(inplace=True)\n",
    "        \n",
    "    # Add timestamp column\n",
    "    df['Timestamp'] = df['Date'].astype(str).apply(date_to_timestamp)\n",
    "        \n",
    "    # Select relevant columns\n",
    "    df = df[[\"Timestamp\", \"Date\", \"RSI Value\"]]\n",
    "        \n",
    "    df = flatten_columns(df)\n",
    "    df = df.iloc[window - 1:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880cf080-5a38-4fc8-9c98-029f7ef546e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_williams_r(stock_data, window=14):\n",
    "    \"\"\"\n",
    "    Calculates Williams %R for the given stock data.\n",
    "    \n",
    "    - Above -20 â†’ Overbought (Sell signal)\n",
    "    - Below -80 â†’ Oversold (Buy signal)\n",
    "    \n",
    "    Parameters:\n",
    "    - stock_data: DataFrame containing \"High\", \"Low\", and \"Close\" columns.\n",
    "    - window: The period over which Williams %R is calculated (default = 14 days).\n",
    "        \n",
    "    Returns:\n",
    "    - DataFrame with an additional \"Williams %R\" column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the highest high and lowest low over the window period\n",
    "    highest_high = stock_data[\"High\"].rolling(window=window, min_periods=1).max()\n",
    "    lowest_low = stock_data[\"Low\"].rolling(window=window, min_periods=1).min()\n",
    "    \n",
    "    # Calculate Williams %R\n",
    "    stock_data[\"Williams %R\"] = ((highest_high - stock_data[\"Close\"]) / \n",
    "                                 (highest_high - lowest_low)) * -100\n",
    "\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c8b457-0eda-42a5-8b19-b895d8f3521d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_KDJ(stock_data, window_k_raw=9, window_d=3, window_k_smooth=3):\n",
    "    \"\"\"\n",
    "    Calculates the KDJ indicators for a given stock dataset.\n",
    "\n",
    "    - %K = Smoothed RSV (Relative Strength Value)\n",
    "    - %D = Moving Average of %K\n",
    "    - %J = 3 * %K - 2 * %D (momentum signal)\n",
    "\n",
    "    Parameters:\n",
    "    - stock_data: DataFrame from `single_stock_table()`\n",
    "    - window_k_raw: Period for RSV calculation (default = 9 days)\n",
    "    - window_d: Period for %D smoothing (default = 3 days)\n",
    "    - window_k_smooth: Period for %K smoothing (default = 3 days)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with %K, %D, and %J indicators added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten column headers if they are multi-indexed\n",
    "    if isinstance(stock_data.columns, pd.MultiIndex):\n",
    "        stock_data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in stock_data.columns]\n",
    "\n",
    "    # Ensure the dataset contains necessary columns (matching single_stock_table())\n",
    "    required_columns = {\"High\", \"Low\", \"Close\"}\n",
    "    if not required_columns.issubset(stock_data.columns):\n",
    "        raise ValueError(f\"Missing required columns: {required_columns - set(stock_data.columns)}. Available columns: {list(stock_data.columns)}\")\n",
    "\n",
    "    # Calculate RSV (Raw Stochastic Value)\n",
    "    stock_data[\"H\"] = stock_data[\"High\"].rolling(window=window_k_raw, min_periods=1).max()\n",
    "    stock_data[\"L\"] = stock_data[\"Low\"].rolling(window=window_k_raw, min_periods=1).min()\n",
    "\n",
    "    # Prevent division by zero (if H == L, replace with 1 to avoid NaN)\n",
    "    stock_data[\"RSV\"] = ((stock_data[\"Close\"] - stock_data[\"L\"]) /\n",
    "                         (stock_data[\"H\"] - stock_data[\"L\"]).replace(0, 1)) * 100\n",
    "\n",
    "    # Calculate %K as a moving average of RSV\n",
    "    stock_data[\"%K\"] = stock_data[\"RSV\"].rolling(window=window_k_smooth, min_periods=1).mean()\n",
    "\n",
    "    # Calculate %D as a moving average of %K\n",
    "    stock_data[\"%D\"] = stock_data[\"%K\"].rolling(window=window_d, min_periods=1).mean()\n",
    "\n",
    "    # Calculate %J as a momentum signal\n",
    "    stock_data[\"%J\"] = 3 * stock_data[\"%K\"] - 2 * stock_data[\"%D\"]\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    stock_data.drop(columns=[\"H\", \"L\", \"RSV\"], inplace=True)\n",
    "\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82073866-df1b-463d-9674-194962c10e87",
   "metadata": {},
   "source": [
    "Finally, our most important function: getting all the features of a given stock and returning a dataframe to be utilized in training the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e655230e-3dbe-487f-ac2b-391d51a2ab9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_features(ticker, start_date, end_date, alpha = 0.8, smoothing = True,\n",
    "                     macd_short_window=12, macd_long_window=26, macd_signal_window=9, \n",
    "                     rsi_window=14, \n",
    "                     williams_r_window=14, \n",
    "                     window_k_raw=9, window_d=3, window_k_smooth=3):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with all stock indicators:\n",
    "    - MACD (Moving Average Convergence Divergence)\n",
    "    - RSI (Relative Strength Index)\n",
    "    - Williams %R (Momentum indicator)\n",
    "    - KDJ (Stock trend analysis)\n",
    "    \"\"\"\n",
    "    \n",
    "    basic_data = single_stock_table(ticker, start_date, end_date)\n",
    "    macd_data = fetch_macd(ticker, start_date, end_date, \n",
    "                           macd_short_window, macd_long_window, macd_signal_window)\n",
    "    rsi_data = fetch_rsi(ticker, start_date, end_date, rsi_window)\n",
    "    \n",
    "    # exponentially smooth data\n",
    "    if smoothing:\n",
    "        basic_data = exponential_smoothing(alpha, basic_data)\n",
    "\n",
    "    #basic_data.rename(columns={\"Date_\": \"Date\"}, inplace=True)\n",
    "    #macd_data.rename(columns={\"Timestamp_\": \"Date\"}, inplace=True)\n",
    "    #rsi_data.rename(columns={\"Timestamp_\": \"Date\"}, inplace=True)\n",
    "\n",
    "    df = pd.merge(basic_data, macd_data, how=\"inner\", on=[\"Date\", \"Timestamp\"])\n",
    "    df = pd.merge(df, rsi_data, how=\"inner\", on=[\"Date\", \"Timestamp\"])\n",
    "\n",
    "    high_col = [col for col in df.columns if \"High\" in col][0]\n",
    "    low_col = [col for col in df.columns if \"Low\" in col][0]\n",
    "    close_col = [col for col in df.columns if \"Close\" in col][0]\n",
    "    \n",
    "    df.rename(columns={high_col: \"High\", low_col: \"Low\", close_col: \"Close\"}, inplace=True)\n",
    "\n",
    "    df = calculate_williams_r(df, williams_r_window)\n",
    "    \n",
    "    df = calculate_KDJ(df, window_k_raw, window_d, window_k_smooth)\n",
    "    \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "    \n",
    "    df = df[40:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9528ad2-6685-4810-b527-a5b6398a69af",
   "metadata": {},
   "source": [
    "These functions will be called inside of the functions from the model maker modules, which is why we are not calling them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab6304-4312-4236-b342-5db26aff7c86",
   "metadata": {},
   "source": [
    "# Constructing and Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee268e-0530-4baf-939b-30da16dd68a3",
   "metadata": {},
   "source": [
    "Next, we will demonstrate how to make three different machine learning models: a CNN (Convolutional Neural Network) model, an LSTM (Long Short-Term Memory) model, and a Random Forest Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182352d-b0a1-410f-89aa-56ebf9ea3789",
   "metadata": {},
   "source": [
    "We will start with the CNN. The following functions are already contained in the CNN_maker module, but will also be shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ab77d0-f543-4055-8e53-81268814bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to create training sequences\n",
    "def create_training_sequences(ticker, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5):\n",
    "    \"\"\"\n",
    "    Returns a training set consisting of data from one stock ticker\n",
    "        :param ticker: A string indicating a single stock ticker\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "    \"\"\"\n",
    "    \n",
    "    # identify features used in model\n",
    "    features = ['Close', 'High', 'Low', 'MACD Value', 'Signal', 'RSI Value', '%K', '%D', '%J', 'Williams %R']\n",
    "    \n",
    "    # get dataframe and targets\n",
    "    df = yahoo_interface.get_all_features(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "    df['Target'] = np.sign(df['Close'].shift(-1) - df['Close'])\n",
    "    df = df.dropna()\n",
    "\n",
    "    # define scaler and label encoder\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # initialize outputs\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # parse through chunks\n",
    "    for i in range(len(df)//chunk_size + 1):\n",
    "        # isolate a chunk\n",
    "        df_chunk = df.iloc[i * chunk_size:(i + 1) * chunk_size].copy()\n",
    "        for feature in features:\n",
    "            data_chunk = df_chunk[feature]  # Get features in chunk\n",
    "            scaled_chunk = scaler.fit_transform(data_chunk.values.reshape(-1, 1)).flatten() # Normalize selected feature chunk\n",
    "            df_chunk.loc[data_chunk.index, feature] = scaled_chunk  # Assign to dataframe\n",
    "\n",
    "        # create sequences\n",
    "        for start in range(0, len(df_chunk) - 14):\n",
    "            end = start + 14\n",
    "            window = df_chunk[features].iloc[start: end].to_numpy()\n",
    "            target = df_chunk['Target'].iloc[end]\n",
    "            X.append(window)\n",
    "            y.append(target)\n",
    "\n",
    "    X = np.array(X, dtype = np.float32)\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ed67a71-6c57-47f4-a58f-0c892eea9bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same as create_training_sequences, but accepts multiple tickers\n",
    "def create_training_sets(tickers, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5):\n",
    "    \"\"\"\n",
    "    Returns a training set consisting of data from multiple stock tickers\n",
    "        :param tickers: An array of strings representing stock tickers\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize output arrays\n",
    "    X_train = np.empty((0,14,10), dtype = 'float64')\n",
    "    y_train = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    # add to training set by calling create_training_sequences\n",
    "    for ticker in tickers:\n",
    "      X, y = create_training_sequences(ticker, '2000-01-01', '2025-01-01', alpha = 0.8)\n",
    "      X_train = np.concatenate((X_train, X), axis = 0)\n",
    "      y_train = np.concatenate((y_train, y), axis = 0)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c066bcbc-25b3-4c2b-8676-de1bffc632d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modified version of create_training_sequences which does not output target labels\n",
    "def create_test_sequences(ticker, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5):\n",
    "    \"\"\"\n",
    "    Returns a test set consisting of data from one stock ticker\n",
    "        :param ticker: A string indicating a single stock ticker\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "    \"\"\"\n",
    "    \n",
    "    # identify features used in model\n",
    "    features = ['Close', 'High', 'Low', 'MACD Value', 'Signal', 'RSI Value', '%K', '%D', '%J', 'Williams %R']\n",
    "    \n",
    "    # get dataframe and targets\n",
    "    df = yahoo_interface.get_all_features(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "    df['Target'] = np.sign(df['Close'].shift(-1) - df['Close'])\n",
    "    df = df.dropna()\n",
    "\n",
    "    # define scaler and label encoder\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # initialize output\n",
    "    X = []\n",
    "\n",
    "    # parse through chunks\n",
    "    for i in range(len(df)//chunk_size + 1):\n",
    "        # isolate a chunk\n",
    "        df_chunk = df.iloc[i * chunk_size:(i + 1) * chunk_size].copy()\n",
    "        for feature in features:\n",
    "            data_chunk = df_chunk[feature]  # Get features in chunk\n",
    "            scaled_chunk = scaler.fit_transform(data_chunk.values.reshape(-1, 1)).flatten() # Normalize selected feature chunk\n",
    "            df_chunk.loc[data_chunk.index, feature] = scaled_chunk  # Assign to dataframe\n",
    "\n",
    "        # create sequences\n",
    "        for start in range(0, len(df_chunk) - 14):\n",
    "            end = start + 14\n",
    "            window = df_chunk[features].iloc[start: end].to_numpy()\n",
    "            target = df_chunk['Target'].iloc[end]\n",
    "            X.append(window)\n",
    "\n",
    "    X = np.array(X, dtype = np.float32)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31f7727e-cc64-49a0-8c93-d49f49ac2773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_CNN(imported_weights = None, training_set = None):\n",
    "    \"\"\"\n",
    "    Creates and returns a CNN model trained in one of two ways:\n",
    "        if imported_weights is specified, then weights are loaded in using model.load_weights()\n",
    "        if training_set is specified, then model is trained using model.fit()\n",
    "    If both ways are specified, then imported_weights takes precedence\n",
    "    If none of the two ways are specified, then method calls create_training_sets\n",
    "    to create a training set.\n",
    "        :param imported_weights: a string ending in '.weights.h5', indicating the name of a weights file which is compatible with the architecture of the LSTM model below\n",
    "        :param training_set: a tuple of the form (X_train, y_train), where X_train is a numpy array of size (n, 14, 10) and y_train is of size (n,), where n is the number of records in the training data\n",
    "    \"\"\"\n",
    "    \n",
    "    # create model\n",
    "    model = keras.Sequential([\n",
    "        layers.Input((14, 10, 1)),\n",
    "        layers.Conv2D(32, (3, 3), padding = 'same', activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), padding = 'same', activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), padding = 'same',activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(2)\n",
    "    ])\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam',\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics = ['accuracy'])\n",
    "    \n",
    "    # if weights specified, import\n",
    "    if imported_weights != None:\n",
    "        model.load_weights(imported_weights)\n",
    "    # if training set specified, fit to training data\n",
    "    elif training_set != None:\n",
    "        model.fit(training_set[0], training_set[1], epochs = 50)\n",
    "    # otherwise, train with the following defualt data set\n",
    "    else:\n",
    "        tickers = ['AAPL', 'COST', 'CVX', 'WM', 'LLY']\n",
    "        start_date = '2000-01-01' \n",
    "        end_date = '2025-03-10'\n",
    "        X_train, y_train = create_training_sets(tickers, start_date, end_date, \n",
    "                                                chunk_size = 365, smoothing = True, alpha = 0.5)\n",
    "        model.fit(X_train, y_train, epochs = 50)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b353e3ac-3413-47e8-95aa-3c52989349c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_Wrapper:\n",
    "    def __init__(self, imported_weights = None, training_set = None):\n",
    "        self.model = create_CNN(imported_weights, training_set)\n",
    "        \n",
    "    def evaluate(self, ticker, start_date, end_date, smoothing = True, alpha = 0.5):\n",
    "        \"\"\"\n",
    "        Tests model accuracy against stock data on a specified ticker over a specified range\n",
    "            :param ticker: A string indicating a single stock ticker\n",
    "            :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "            :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "            :param chunk_size: An integer specifying the number of records in each chunk \n",
    "            :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "            :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "        \"\"\"\n",
    "        \n",
    "        # create test set\n",
    "        X_test, y_test = create_training_sequences(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "        \n",
    "        # evaluate model\n",
    "        self.model.evaluate(X_test, y_test)\n",
    "\n",
    "    def predict(self, ticker):\n",
    "        \"\"\"\n",
    "        Predicts whether or not a stock's current closing price will rise or fall tomorrow\n",
    "            :param ticker: a string specifying a stock ticker\n",
    "        \"\"\"\n",
    "        \n",
    "        # identify classes\n",
    "        classes = ['Rise', 'Fall']\n",
    "        \n",
    "        # get today's timestamp\n",
    "        current_timestamp = datetime.now().timestamp()*1000\n",
    "        \n",
    "        # get timestamp for 5 months back (used to compute features)\n",
    "        past_timestamp = current_timestamp - 86400 * 31 * 5 * 1000\n",
    "        \n",
    "        # convert timestamps to strings of form YYYY-MM-DD\n",
    "        current_date = yahoo_interface.timestamp_to_date(current_timestamp)\n",
    "        past_date = yahoo_interface.timestamp_to_date(past_timestamp)\n",
    "        \n",
    "        # get sequences to be fed into model\n",
    "        X_test = create_test_sequences(ticker, past_date, current_date, smoothing = False)\n",
    "        \n",
    "        # get predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # apply argmax \n",
    "        labels = y_pred.argmax(axis = 1)\n",
    "        \n",
    "        # output prediction\n",
    "        return classes[labels.item(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d264488-bf1e-4cc9-a69c-93c2e97e5381",
   "metadata": {},
   "source": [
    "Next is the LSTM model (again, these functions are all found in the LSTM_Maker module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "202d7b01-d6ed-4f18-b986-3f6fd04732f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to keep values in [0,1]\n",
    "def proj(x, a, b):\n",
    "  \"\"\"\n",
    "  Projects a number x onto the interval [a,b]\n",
    "  Values in [a,b] are projected onto themselves\n",
    "  Values less than a are projected onto a\n",
    "  Values greater than b are projected onto b\n",
    "    :param x: number to be projected onto [a,b]\n",
    "    :param a: left endpoint of interval [a,b]\n",
    "    :param b: right endpoint of interval [a,b]\n",
    "  \"\"\"\n",
    "  return max(min(x, b), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dac2fc1-948c-489d-bfe8-296e805e202c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to add noise to stock predictions\n",
    "def noise():\n",
    "    \"\"\"\n",
    "    Takes one sample of a normal distribution with mean 0, stdev = 0.33, projected onto [-1,1]\n",
    "    \"\"\"\n",
    "    value = np.random.normal(0, 0.33, 1)\n",
    "    return proj(value, -1,1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a64ca0-70e6-47f0-b5b8-2560cb1d11f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for creating training sequences to feed into model\n",
    "def create_training_sequences(data, window_size = 60):\n",
    "    \"\"\"\n",
    "      Returns a list of windows in data of length window_size\n",
    "          and a list containing corresponding to the data value immediately proceeding a window\n",
    "\n",
    "      :param data: a numpy array of shape (n, 1), e.g. closing_prices\n",
    "      :param window_size: length of window\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize output lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # take observations\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i: i + window_size, 0])    # closing prices in last (seq_length)-days\n",
    "        y.append(data[i + window_size, 0])       # closing price in day after the x-sequence\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac0dde5-6d51-463e-b451-70726f747a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for creating training sets\n",
    "def create_training_sets(tickers, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5):\n",
    "    \"\"\"\n",
    "    Returns a training set and associated targets.    \n",
    "        :param tickers: An array of strings representing stock tickers. This specifies which stocks to get observations from.\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "    \"\"\"\n",
    "    \n",
    "    window_size = 60\n",
    "    X_train = np.empty((0,60), dtype = 'float64')\n",
    "    y_train = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "      df = yahoo_interface.get_all_features(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "      for i in range(len(df)//chunk_size):\n",
    "        # get chunk of dataframe\n",
    "        df_chunk = df[i*chunk_size: min((i+1)*chunk_size, len(df))]\n",
    "    \n",
    "        # Put closing prices into numpy array\n",
    "        closing_prices = df_chunk['Close'].values.reshape(-1, 1)\n",
    "    \n",
    "        # Normalize the data between [0,1]\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        scaled_data = scaler.fit_transform(closing_prices)\n",
    "        \n",
    "        X, y = create_training_sequences(scaled_data, window_size)\n",
    "        X_train = np.concatenate((X_train, X))\n",
    "        y_train = np.concatenate((y_train, y))\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4a1de92-9612-4ae2-90c9-099c4d5f4db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for creating training/validation sets\n",
    "def create_training_and_val_sets(tickers, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5, split = 0.7):\n",
    "    \"\"\"\n",
    "    Returns a training and validation sets.    \n",
    "        :param tickers: An array of strings representing stock tickers. This specifies which stocks to get observations from.\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "        :param split: A float in [0,1] indicating what portion of the data is to be used for training vs validation\n",
    "    \"\"\"\n",
    "    \n",
    "    window_size = 60\n",
    "    X_train = np.empty((0,60), dtype = 'float64')\n",
    "    y_train = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    X_val = np.empty((0,60), dtype = 'float64')\n",
    "    y_val = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "      df = yahoo_interface.get_all_features(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "      for i in range(len(df)//chunk_size):\n",
    "        # get chunk of dataframe\n",
    "        df_chunk = df[i*chunk_size: min((i+1)*chunk_size, len(df))]\n",
    "    \n",
    "        # Put closing prices into numpy array\n",
    "        closing_prices = df_chunk['Close'].values.reshape(-1, 1)\n",
    "    \n",
    "        # Normalize the data between [0,1]\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        scaled_data = scaler.fit_transform(closing_prices)\n",
    "    \n",
    "        train_size = int(len(df_chunk) * split)\n",
    "        train, val = scaled_data[: train_size, :], scaled_data[train_size:, :]\n",
    "        \n",
    "        X, y = create_training_sequences(train, window_size)\n",
    "        X_train = np.concatenate((X_train, X))\n",
    "        y_train = np.concatenate((y_train, y))\n",
    "        \n",
    "        X, y = create_training_sequences(val, window_size)\n",
    "        X_val = np.concatenate((X_val, X))\n",
    "        y_val = np.concatenate((y_val, y))\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33b8364a-7e3a-4298-8415-04ac9298ef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modified version of create_training_sequences. Only outputs X array, not y array\n",
    "def create_sequences(data, window_size = 60):\n",
    "    \n",
    "    \"\"\"\n",
    "      Returns a list of windows in data of length window_size\n",
    "          and a list containing corresponding to the data value immediately proceeding a window\n",
    "\n",
    "      :param data: a numpy array of shape (n, 1), e.g. closing_prices\n",
    "      :param window_size: length of window\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        X.append(data[i: i + window_size, 0])    # closing prices in last (seq_length)-days\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b86e476-effa-4d8a-988c-b330ad39e77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creates LSTM model\n",
    "def create_LSTM(imported_weights = None, training_set = None):\n",
    "    \"\"\"\n",
    "    Creates and returns an LSTM model trained in one of two ways:\n",
    "        if imported_weights is specified, then weights are loaded in using model.load_weights()\n",
    "        if training_set is specified, then model is trained using model.fit()\n",
    "    If both ways are specified, then imported_weights takes precedence\n",
    "    If none of the two ways are specified, then method calls create_training_sets\n",
    "    to create a training set.\n",
    "        :param imported_weights: a string ending in '.weights.h5', indicating the name of a weights file which is compatible with the architecture of the LSTM model below\n",
    "        :param training_set: a tuple of the form (X_train, y_train), where X_train is a numpy array of size (n, 60) and y_train is of size (n,), where n is the number of records in the training data\n",
    "    \"\"\"\n",
    "    \n",
    "    # create model\n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(32, return_sequences = True, input_shape = (60, 1)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation = 'relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    \n",
    "    # if weights specified, import and return\n",
    "    if imported_weights != None:\n",
    "        model.load_weights(imported_weights)\n",
    "    elif training_set != None:\n",
    "        model.fit(training_set[0], training_set[1], epochs = 50)\n",
    "    else:\n",
    "        tickers = ['AAPL', 'COST', 'CVX', 'WM', 'LLY']\n",
    "        start_date = '2000-01-01' \n",
    "        end_date = '2025-03-10'\n",
    "        X_train, y_train = create_training_sets(tickers, start_date, end_date, \n",
    "                                                chunk_size = 365, smoothing = True, alpha = 0.5)\n",
    "        model.fit(X_train, y_train, epochs = 50)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2833e71-1f48-4797-9345-79417362bf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_wrapper:\n",
    "    def __init__(self, imported_weights = None, training_set = None):\n",
    "        self.model = create_LSTM(imported_weights, training_set)\n",
    "        \n",
    "    def line_plot_evaluation(self, tickers, start_date, end_date):\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        \n",
    "        plt.figure(figsize = (20,10))\n",
    "        for i in range(6):\n",
    "          # reading data\n",
    "          df = yahoo_interface.get_all_features(f'{tickers[i]}', start_date, end_date, smoothing = False)\n",
    "          closing_prices = df['Close'].values.reshape(-1, 1)\n",
    "          scaled_data = scaler.fit_transform(closing_prices)\n",
    "        \n",
    "          # formatting data to be fed into model\n",
    "          window_size = 60\n",
    "          X_full = create_sequences(scaled_data, window_size)\n",
    "        \n",
    "          # getting predictions\n",
    "          full_predict = self.model.predict(X_full)\n",
    "          full_predict = scaler.inverse_transform(full_predict)\n",
    "          mse = mean_squared_error(closing_prices[59:], full_predict)\n",
    "        \n",
    "          # Plotting real vs predicted prices\n",
    "          known_plot = np.empty((closing_prices.shape[0] + 1, 1))\n",
    "          known_plot[:, :] = np.nan\n",
    "          known_plot[:closing_prices.shape[0], :] = closing_prices\n",
    "        \n",
    "          prediction_plot = np.empty((closing_prices.shape[0] + 1, 1))\n",
    "          prediction_plot[:, :] = np.nan\n",
    "          prediction_plot[window_size: closing_prices.shape[0] + 1, :] = full_predict\n",
    "        \n",
    "          # set up subplots\n",
    "          plt.subplot(2,3,i+1)\n",
    "          plt.title(f'{tickers[i]}')\n",
    "          plt.plot(known_plot, color = 'blue', label = 'Actual Price')\n",
    "          plt.plot(prediction_plot, color = 'orange', label = f'Predicted Price (MSE = {mse:.4f})')\n",
    "          plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def forecast(self, stock_data, days_forward = 60):\n",
    "        \"\"\"\n",
    "        Generates predictions for a stock given 60 days of historical data\n",
    "        Returns an appended version of scaled_data with predictions appearing at the end of the array\n",
    "            :param stock_data: a dataframe containing a column titled 'Close', with at least 60 rows\n",
    "            :param days_forward: the number of days in the future we want to forecast\n",
    "        \"\"\"\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        scaled_data = scaler.fit_transform(stock_data['Close'].values.reshape(-1, 1))\n",
    "        \n",
    "        # initialize array to store future values\n",
    "        forecast_arr = scaled_data\n",
    "        \n",
    "        # get predictions\n",
    "        for i in range(days_forward):\n",
    "            # get last 60 days in array\n",
    "            seq = create_sequences(forecast_arr[-60:], 60)\n",
    "        \n",
    "            # predict next day using model\n",
    "            one_predict = self.model.predict(seq, verbose = 0)\n",
    "        \n",
    "            # add noise to prediction\n",
    "            updated_value = np.array(one_predict[0][0].item()*(1 + 0.1 * noise()))\n",
    "            one_predict[0][0] = updated_value.item()\n",
    "        \n",
    "            # add prediction to array\n",
    "            forecast_arr = np.concatenate((forecast_arr, one_predict))\n",
    "        \n",
    "        # reverse data scaling\n",
    "        forecast_arr = scaler.inverse_transform(forecast_arr)\n",
    "        \n",
    "        return forecast_arr\n",
    "        \n",
    "    def forecast_ensemble(self, stock_data, days_forward = 60, size = 10):\n",
    "        \"\"\"\n",
    "        Calls forecast() several times and returns a list of forecast arrays\n",
    "            :param stock_data: a dataframe containing a column titled 'Close', with at least 60 rows\n",
    "            :param days_forward: the number of days in the future we want to forecast\n",
    "            :param size: size of output list, i.e. number of times forecast() is called\n",
    "        \"\"\"\n",
    "        forecast_list = []\n",
    "        for i in range(size):\n",
    "            forecast_list.append(self.forecast(stock_data = stock_data, days_forward = days_forward))\n",
    "\n",
    "        return forecast_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b020c1-cf1e-4cc4-909f-97691f76d524",
   "metadata": {},
   "source": [
    "Just like the previous two models, the functions for the Random Forest model are found in the Random_Forest_Maker module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e3114ad-dd22-4743-85c1-edfed6cb2ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to keep values in [0,1]\n",
    "def proj(x, a, b):\n",
    "  \"\"\"\n",
    "  Projects a number x onto the interval [a,b]\n",
    "  Values in [a,b] are projected onto themselves\n",
    "  Values less than a are projected onto a\n",
    "  Values greater than b are projected onto b\n",
    "    :param x: number to be projected onto [a,b]\n",
    "    :param a: left endpoint of interval [a,b]\n",
    "    :param b: right endpoint of interval [a,b]\n",
    "  \"\"\"\n",
    "  return max(min(x, b), a)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1882c1c-e1f9-4c23-a812-af43b3a4d757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to add noise to stock predictions\n",
    "def noise():\n",
    "    \"\"\"\n",
    "    Takes one sample of a normal distribution with mean 0, stdev = 0.33, projected onto [-1,1]\n",
    "    \"\"\"\n",
    "    value = np.random.normal(0, 0.33, 1)\n",
    "    return proj(value, -1,1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e0ac907-4bc3-47a7-8836-dc47256e8d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for creating training sequences to feed into model\n",
    "def create_training_sequences(data, window_size = 60):\n",
    "    \"\"\"\n",
    "      Returns a list of windows in data of length window_size\n",
    "          and a list containing corresponding to the data value immediately proceeding a window\n",
    "\n",
    "      :param data: a numpy array of shape (n, 1), e.g. closing_prices\n",
    "      :param window_size: length of window\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize output lists\n",
    "    X, y = [], []\n",
    "    \n",
    "    # take observations\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i: i + window_size, 0])    # closing prices in last (seq_length)-days\n",
    "        y.append(data[i + window_size, 0])       # closing price in day after the x-sequence\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59916956-feae-47af-ab7a-c34a01ce7c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for creating training sets\n",
    "def create_training_sets(tickers, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5):\n",
    "    \"\"\"\n",
    "    Returns a training set and associated targets.    \n",
    "        :param tickers: An array of strings representing stock tickers. This specifies which stocks to get observations from.\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "    \"\"\"\n",
    "    \n",
    "    window_size = 60\n",
    "    X_train = np.empty((0,60), dtype = 'float64')\n",
    "    y_train = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "      df = yahoo_interface.get_all_features(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "      for i in range(len(df)//chunk_size):\n",
    "        # get chunk of dataframe\n",
    "        df_chunk = df[i*chunk_size: min((i+1)*chunk_size, len(df))]\n",
    "    \n",
    "        # Put closing prices into numpy array\n",
    "        closing_prices = df_chunk['Close'].values.reshape(-1, 1)\n",
    "    \n",
    "        # Normalize the data between [0,1]\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        scaled_data = scaler.fit_transform(closing_prices)\n",
    "        \n",
    "        X, y = create_training_sequences(scaled_data, window_size)\n",
    "        X_train = np.concatenate((X_train, X))\n",
    "        y_train = np.concatenate((y_train, y))\n",
    "\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a99c243c-637e-4770-a25e-98c30fd77a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for creating training/validation sets\n",
    "def create_training_and_val_sets(tickers, start_date, end_date, chunk_size = 365, smoothing = True, alpha = 0.5, split = 0.7):\n",
    "    \"\"\"\n",
    "    Returns a training and validation sets.    \n",
    "        :param tickers: An array of strings representing stock tickers. This specifies which stocks to get observations from.\n",
    "        :param start_date: A string of the form YYYY-MM-DD specifying the start of the date range\n",
    "        :param end_date: A string of the form YYYY-MM-DD specifying the end of the date range\n",
    "        :param chunk_size: An integer specifying the number of records in each chunk \n",
    "        :param smoothing: A boolean indicating whether or not to apply exponential smoothing when calling yahoo_interface.get_all_features()\n",
    "        :param alpha: A float in [0,1], i.e. the exponential smoothing parameter, which is passed to yahoo_interface.get_all_features()\n",
    "        :param split: A float in [0,1] indicating what portion of the data is to be used for training vs validation\n",
    "    \"\"\"\n",
    "    \n",
    "    window_size = 60\n",
    "    X_train = np.empty((0,60), dtype = 'float64')\n",
    "    y_train = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    X_val = np.empty((0,60), dtype = 'float64')\n",
    "    y_val = np.empty((0), dtype = 'float64')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "      df = yahoo_interface.get_all_features(ticker, start_date, end_date, smoothing = smoothing, alpha = alpha)\n",
    "      for i in range(len(df)//chunk_size):\n",
    "        # get chunk of dataframe\n",
    "        df_chunk = df[i*chunk_size: min((i+1)*chunk_size, len(df))]\n",
    "    \n",
    "        # Put closing prices into numpy array\n",
    "        closing_prices = df_chunk['Close'].values.reshape(-1, 1)\n",
    "    \n",
    "        # Normalize the data between [0,1]\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        scaled_data = scaler.fit_transform(closing_prices)\n",
    "    \n",
    "        train_size = int(len(df_chunk) * split)\n",
    "        train, val = scaled_data[: train_size, :], scaled_data[train_size:, :]\n",
    "        \n",
    "        X, y = create_training_sequences(train, window_size)\n",
    "        X_train = np.concatenate((X_train, X))\n",
    "        y_train = np.concatenate((y_train, y))\n",
    "        \n",
    "        X, y = create_training_sequences(val, window_size)\n",
    "        X_val = np.concatenate((X_val, X))\n",
    "        y_val = np.concatenate((y_val, y))\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eec227a3-7b23-438c-8d2f-af505a86cb43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modified version of create_training_sequences. Only outputs X array, not y array\n",
    "def create_sequences(data, window_size = 60):\n",
    "    \"\"\"\n",
    "      Returns a list of windows in data of length window_size\n",
    "          and a list containing corresponding to the data value immediately proceeding a window\n",
    "\n",
    "      :param data: a numpy array of shape (n, 1), e.g. closing_prices\n",
    "      :param window_size: length of window\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        X.append(data[i: i + window_size, 0])    # closing prices in last (seq_length)-days\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88451574-30c2-46c2-bcd7-7e0bc2dfd9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_random_forest(X_train, y_train, \n",
    "                           n_estimators=100, \n",
    "                           max_depth=None, \n",
    "                           min_samples_split=2, \n",
    "                           min_samples_leaf=1, \n",
    "                           random_state=42):\n",
    "    \"\"\"Creates, trains, and returns a RandomForestRegressor model.\"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  min_samples_split=min_samples_split,\n",
    "                                  min_samples_leaf=min_samples_leaf,\n",
    "                                  random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "770df59b-a96d-48d7-add2-40bc23e11114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Random_Forest_wrapper:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.model = create_random_forest(X_train, y_train)\n",
    "   \n",
    "    def mse_evaluation(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Makes predictions and evaluates the model's performance.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.model.predict(X_val)\n",
    "    \n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "        print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "        print(f\"R-squared (R2): {r2}\")\n",
    "    \n",
    "        return y_pred\n",
    "    \n",
    "    def scatter_plot_evaluation(self, y_val, y_pred):\n",
    "        \"\"\"\n",
    "        Creates a scatter plot of actual vs. predicted values.\n",
    "        \"\"\"\n",
    "        plt.scatter(y_val, y_pred)\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(\"Actual vs. Predicted Values (Random Forest Regression)\")\n",
    "        plt.show()\n",
    "        \n",
    "    def line_plot_evaluation(self, tickers, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Creates a line plot comparing actual vs predicted prices against multiple stocks\n",
    "        \"\"\"\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        \n",
    "        plt.figure(figsize = (15,10))\n",
    "        for i in range(6):\n",
    "            # reading data\n",
    "            df = yahoo_interface.get_all_features(f'{tickers[i]}', '2024-01-01', '2025-01-01', smoothing = False)\n",
    "            closing_prices = df['Close'].values\n",
    "            closing_prices = closing_prices[:,np.newaxis]\n",
    "            scaled_data = scaler.fit_transform(closing_prices)\n",
    "        \n",
    "            # formatting data to be fed into model\n",
    "            window_size = 60\n",
    "            X_full = create_sequences(scaled_data, window_size)\n",
    "        \n",
    "            # getting predictions\n",
    "            full_predict = self.model.predict(X_full)\n",
    "            full_predict = full_predict[:,np.newaxis]\n",
    "            full_predict = scaler.inverse_transform(full_predict)\n",
    "        \n",
    "            # Plotting real vs predicted prices\n",
    "            known_plot = np.empty((closing_prices.shape[0] + 1, 1))\n",
    "            known_plot[:, :] = np.nan\n",
    "            known_plot[:closing_prices.shape[0], :] = closing_prices\n",
    "        \n",
    "            prediction_plot = np.empty((closing_prices.shape[0] + 1, 1))\n",
    "            prediction_plot[:, :] = np.nan\n",
    "            prediction_plot[window_size: closing_prices.shape[0] + 1, :] = full_predict\n",
    "        \n",
    "            # set up subplots\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.title(f'{tickers[i]}')\n",
    "            plt.plot(known_plot, color = 'blue', label = 'Actual Price')\n",
    "            plt.plot(prediction_plot, color = 'orange', label = 'Predicted Price')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def forecast(self, stock_data, days_forward = 60):\n",
    "        \"\"\"\n",
    "        Generates predictions for a stock given 60 days of historical data\n",
    "        Returns an appended version of scaled_data with predictions appearing at the end of the array\n",
    "            :param stock_data: a dataframe containing a column titled 'Close', with at least 60 rows\n",
    "            :param days_forward: the number of days in the future we want to forecast\n",
    "        \"\"\"\n",
    "        scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "        scaled_data = scaler.fit_transform(stock_data['Close'].values.reshape(-1, 1))\n",
    "        \n",
    "        # initialize array to store future values\n",
    "        forecast_arr = scaled_data\n",
    "        \n",
    "        # get predictions\n",
    "        for i in range(days_forward):\n",
    "            seq = create_sequences(forecast_arr[-60:], 60)\n",
    "            one_predict = self.model.predict(seq)\n",
    "            one_predict[0] = one_predict[0]*(1 + 0.1 * noise())\n",
    "            one_predict = one_predict[:,np.newaxis]\n",
    "            forecast_arr = np.concatenate((forecast_arr, one_predict))\n",
    "        \n",
    "        forecast_arr = scaler.inverse_transform(forecast_arr)\n",
    "        \n",
    "        return forecast_arr\n",
    "        \n",
    "    def forecast_ensemble(self, stock_data, days_forward = 60, size = 10):\n",
    "        \"\"\"\n",
    "        Calls forecast() several times and returns a list of forecast arrays\n",
    "            :param stock_data: a dataframe containing a column titled 'Close', with at least 60 rows\n",
    "            :param days_forward: the number of days in the future we want to forecast\n",
    "            :param size: size of output list, i.e. number of times forecast() is called\n",
    "        \"\"\"\n",
    "        forecast_list = []\n",
    "        for i in range(size):\n",
    "            forecast_list.append(self.forecast(stock_data = stock_data, days_forward = days_forward))\n",
    "\n",
    "        return forecast_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210e670-e1e6-4f46-a38b-d16df6209fe5",
   "metadata": {},
   "source": [
    "# Getting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d5468-f8df-42d9-b6b0-96d9488bf880",
   "metadata": {},
   "source": [
    "Now, finally, we can use the functions and classes that we made to predict stock trends with the CNN model and stock prices with the LSTM and Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ecc7a-8c08-408c-b1f5-ddd811caaf0d",
   "metadata": {},
   "source": [
    "Let's start with a CNN model to predict if a stock price will rise or fall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73705584-cf5c-47e2-9a1f-f30fa5a8a53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#Create the training set using Apple data\n",
    "CNN_X_train, CNN_y_train = CNN_Maker.create_training_sequences('AAPL', '2000-01-01', '2025-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dd3435b-d479-4052-993b-cc9389d41727",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [CNN_X_train, CNN_y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30518235-6059-42b7-ab90-bdcab82338de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5527 - loss: 0.6880\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5716 - loss: 0.6785\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5899 - loss: 0.6733\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5899 - loss: 0.6695\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6029 - loss: 0.6616\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6049 - loss: 0.6624\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5986 - loss: 0.6646\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5908 - loss: 0.6648\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5917 - loss: 0.6642\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6087 - loss: 0.6602\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6110 - loss: 0.6571\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6171 - loss: 0.6518\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6149 - loss: 0.6512\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6178 - loss: 0.6533\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6190 - loss: 0.6490\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6283 - loss: 0.6417\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6171 - loss: 0.6491\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6204 - loss: 0.6447\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6244 - loss: 0.6430\n",
      "Epoch 20/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6309 - loss: 0.6356\n",
      "Epoch 21/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6262 - loss: 0.6357\n",
      "Epoch 22/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6384 - loss: 0.6373\n",
      "Epoch 23/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6465 - loss: 0.6276\n",
      "Epoch 24/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6312 - loss: 0.6349\n",
      "Epoch 25/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6462 - loss: 0.6228\n",
      "Epoch 26/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6429 - loss: 0.6257\n",
      "Epoch 27/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6620 - loss: 0.6152\n",
      "Epoch 28/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6504 - loss: 0.6219\n",
      "Epoch 29/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6535 - loss: 0.6202\n",
      "Epoch 30/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6546 - loss: 0.6206\n",
      "Epoch 31/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6562 - loss: 0.6149\n",
      "Epoch 32/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6617 - loss: 0.6147\n",
      "Epoch 33/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6612 - loss: 0.6116\n",
      "Epoch 34/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6675 - loss: 0.6027\n",
      "Epoch 35/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6627 - loss: 0.5990\n",
      "Epoch 36/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6674 - loss: 0.5939\n",
      "Epoch 37/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6748 - loss: 0.5946\n",
      "Epoch 38/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6687 - loss: 0.5937\n",
      "Epoch 39/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6771 - loss: 0.5915\n",
      "Epoch 40/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6697 - loss: 0.5946\n",
      "Epoch 41/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6836 - loss: 0.5872\n",
      "Epoch 42/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6800 - loss: 0.5874\n",
      "Epoch 43/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6847 - loss: 0.5768\n",
      "Epoch 44/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6826 - loss: 0.5775\n",
      "Epoch 45/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6801 - loss: 0.5782\n",
      "Epoch 46/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7010 - loss: 0.5725\n",
      "Epoch 47/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6961 - loss: 0.5660\n",
      "Epoch 48/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6817 - loss: 0.5695\n",
      "Epoch 49/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6989 - loss: 0.5623\n",
      "Epoch 50/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7071 - loss: 0.5591\n"
     ]
    }
   ],
   "source": [
    "#Creating a CNN_Wrapper will automatically create a model, compile it, and train it with the training set.\n",
    "CNN_wrapper = CNN_Maker.CNN_Wrapper(training_set = training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b360e71b-e064-4306-b3da-9c85cd879b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m187/187\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5450 - loss: 0.7765\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model against Costco data\n",
    "CNN_wrapper.evaluate('COST', '2000-01-01', '2025-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce3350-ec92-4c13-ad4e-759b9a48c1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad39a21a-27d3-4eea-a144-6c73e8380410",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6c99d-7758-4d00-b866-2387bb51c2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B-25W] *",
   "language": "python",
   "name": "conda-env-PIC16B-25W-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
